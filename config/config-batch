# !mandatory! task to execute, operation available: "traning" (for embedding files), "match" (for ER task), "test" (for verify accuracy of matching) 
######### input config:
# task type: {train, test, match, train-test, train-match}
task:batch
write_edges:false
dataset_file:/home/zhongwei/Data_ingestion/Embdi/EmbDI datasets/er datasets/amazon_google-tableA.csv
# ######### for pre-training of stream
# output_file:amazon_google-tableA-batch
output_file_name:amazon_google-tableA_013
log_path:pipeline/logging

########## graph config:
### tn: token as number,### tt: token as text, ### idx: id of row, ### cid: id of column
### $ represent this value is a string, # represente number
### at the beginning, the number (1-7) represent class of this type of value, definition of different classes is written at file README
node_types:["7#__tn", "7$__tt", "3$__idx", "1$__cid"]
### [The graph algorithm will split all nodes with a prefix listed here]
flatten:tt
##### smooth: smooth/inverse_smooth/log/inverse/no
smoothing_method:no
directed:false

########### walks config:
write_walks:true
sentence_length:100
# Whether backtracking is allowed (whether it is possible to return to the previous node).
backtrack:false
random_walks_per_node:20
rw_stat:true

########## Embeddings configuration:
learning_method:skipgram
window_size:3
n_dimensions:300
# choices for training algo : word2vec, fasttext, doc2vec
training_algorithm:word2vec